# æœ¬åœ°ç®€åŒ–å®éªŒè¿è¡ŒæŒ‡å—

ç”±äºäº‘æœåŠ¡å™¨ç¯å¢ƒä¸å…¼å®¹ï¼ˆCUDAç‰ˆæœ¬ï¼‰ï¼Œè¿™é‡Œæä¾›æœ¬åœ°ç®€åŒ–è¿è¡Œæ–¹æ¡ˆã€‚

## ğŸ¯ ç®€åŒ–ç­–ç•¥

### åŸå§‹é…ç½® vs æœ¬åœ°é…ç½®

| é…ç½®é¡¹ | åŸå§‹ï¼ˆ5090ï¼‰ | ç®€åŒ–ï¼ˆæœ¬åœ°ï¼‰ | è¯´æ˜ |
|--------|-------------|-------------|------|
| **ç¡¬ä»¶** | RTX 5090 | CPU/ä»»æ„GPU | è‡ªåŠ¨æ£€æµ‹ |
| **Batch Size** | 512 | 32 | å‡å°‘å†…å­˜ |
| **Hidden Dim** | 256 | 64 | æ›´å°æ¨¡å‹ |
| **KG Emb Dim** | 256 | 64 | æ›´å°åµŒå…¥ |
| **Num Layers** | 3 | 1 | å•å±‚GRU |
| **Candidates** | 200 | 50 | æ›´å°‘å€™é€‰ |
| **N Steps** | 4096 | 512 | æ›´çŸ­rollout |
| **N Epochs** | 15 | 4 | æ›´å°‘epoch |
| **Timesteps** | 1,000,000 | 10,000 | å¿«é€Ÿæµ‹è¯• |

**æ¨¡å‹å¤§å°**: 4.7M â†’ 0.2M å‚æ•° (23Ã— æ›´å°)
**æ˜¾å­˜éœ€æ±‚**: 5.8GB â†’ 0.5GB (11Ã— æ›´å°)
**è®­ç»ƒæ—¶é—´**: 4å°æ—¶ â†’ 10-20åˆ†é’Ÿ (12-24Ã— æ›´å¿«)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒæ£€æŸ¥

```bash
# æ£€æŸ¥Pythonç¯å¢ƒ
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import numpy; print(f'NumPy: {numpy.__version__}')"

# æ£€æŸ¥CUDAï¼ˆå¯é€‰ï¼‰
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

### 2. å‡†å¤‡æ•°æ®

```bash
# è‡ªåŠ¨ä¸‹è½½å’Œå¤„ç†æ•°æ®
python scripts/prepare_data.py --dataset amazon-book
```

**æ•°æ®æ¥æº**: [KGAT GitHub](https://github.com/xiangwang1223/knowledge_graph_attention_network/tree/master/Data)

**å¦‚æœä¸‹è½½å¤±è´¥**ï¼Œæ‰‹åŠ¨ä¸‹è½½ï¼š
1. è®¿é—®ä¸Šè¿°é“¾æ¥
2. ä¸‹è½½ `amazon-book.zip`
3. è§£å‹åˆ° `data/amazon-book/`

### 3. è¿è¡Œç®€åŒ–è®­ç»ƒ

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ2åˆ†é’Ÿï¼ŒéªŒè¯ç¯å¢ƒï¼‰
python train_local_simplified.py --total-timesteps 1024

# æ ‡å‡†æœ¬åœ°è®­ç»ƒï¼ˆ10-20åˆ†é’Ÿï¼Œè·å–åŸºæœ¬ç»“æœï¼‰
python train_local_simplified.py --total-timesteps 10000

# æ›´é•¿è®­ç»ƒï¼ˆ1-2å°æ—¶ï¼Œæ›´å¥½ç»“æœï¼‰
python train_local_simplified.py --total-timesteps 50000
```

### 4. æŸ¥çœ‹ç»“æœ

```bash
# ç»“æœä¿å­˜åœ¨
cat checkpoints_local/training_results.json
```

## ğŸ“Š é¢„æœŸç»“æœèŒƒå›´

åŸºäºç®€åŒ–é…ç½®ï¼Œé¢„æœŸæ€§èƒ½èŒƒå›´ï¼š

| æŒ‡æ ‡ | ç®€åŒ–æ¨¡å‹ | å®Œæ•´æ¨¡å‹ | å·®è· |
|------|---------|---------|------|
| Recall@20 | 0.065-0.072 | 0.0856 | ~16% |
| NDCG@20 | 0.052-0.058 | 0.0645 | ~13% |
| Training Time | 15 min | 4 hours | 16Ã— |

**è¯´æ˜**: ç®€åŒ–æ¨¡å‹ä»èƒ½å±•ç¤ºæ–¹æ³•æœ‰æ•ˆæ€§ï¼Œä½†ç»å¯¹æ€§èƒ½ä¼šé™ä½ã€‚

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

```bash
# CPUè¿è¡Œï¼ˆæ…¢ä½†å…¼å®¹æ€§å¥½ï¼‰
python train_local_simplified.py --device cpu --total-timesteps 5000

# GPUè¿è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
python train_local_simplified.py --device cuda --total-timesteps 20000

# è°ƒæ•´æ¨¡å‹å¤§å°
python train_local_simplified.py \
    --hidden-dim 128 \
    --kg-emb-dim 128 \
    --batch-size 64

# è°ƒæ•´è®­ç»ƒå‚æ•°
python train_local_simplified.py \
    --lr 1e-3 \
    --n-steps 1024 \
    --n-epochs 8
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å¦‚æœè®­ç»ƒå¤ªæ…¢

1. **å‡å°‘timesteps**:
   ```bash
   python train_local_simplified.py --total-timesteps 2048
   ```

2. **å‡å°‘è¯„ä¼°é¢‘ç‡**:
   ```bash
   python train_local_simplified.py --eval-freq 10
   ```

3. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹**:
   ```bash
   python train_local_simplified.py --hidden-dim 32 --kg-emb-dim 32
   ```

### å¦‚æœå†…å­˜ä¸è¶³

1. **å‡å°batch size**:
   ```bash
   python train_local_simplified.py --batch-size 16
   ```

2. **å‡å°‘å€™é€‰é›†**:
   ```bash
   python train_local_simplified.py --candidate-size 25
   ```

3. **é™åˆ¶è®­ç»ƒåºåˆ—**:
   ä¿®æ”¹ä»£ç ä¸­çš„ `train_seqs = dict(list(train_seqs.items())[:2000])`

## ğŸ“ å®Œæ•´å‚æ•°åˆ—è¡¨

```bash
python train_local_simplified.py --help

å¯ç”¨å‚æ•°:
  --dataset           æ•°æ®é›†åç§° (é»˜è®¤: amazon-book)
  --data-path         æ•°æ®ç›®å½• (é»˜è®¤: ./data/)
  --item-emb-dim      ç‰©å“åµŒå…¥ç»´åº¦ (é»˜è®¤: 64)
  --kg-emb-dim        KGåµŒå…¥ç»´åº¦ (é»˜è®¤: 64)
  --hidden-dim        éšè—å±‚ç»´åº¦ (é»˜è®¤: 64)
  --num-layers        GRUå±‚æ•° (é»˜è®¤: 1)
  --num-hash-bits     LSHå“ˆå¸Œä½æ•° (é»˜è®¤: 6)
  --num-tables        LSHè¡¨æ•° (é»˜è®¤: 2)
  --candidate-size    å€™é€‰é›†å¤§å° (é»˜è®¤: 50)
  --lr                å­¦ä¹ ç‡ (é»˜è®¤: 3e-4)
  --gamma             æŠ˜æ‰£å› å­ (é»˜è®¤: 0.99)
  --gae-lambda        GAE lambda (é»˜è®¤: 0.95)
  --clip-range        PPOè£å‰ªèŒƒå›´ (é»˜è®¤: 0.2)
  --total-timesteps   æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤: 10000)
  --n-steps           æ¯æ¬¡rolloutæ­¥æ•° (é»˜è®¤: 512)
  --batch-size        æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)
  --n-epochs          æ¯æ¬¡æ›´æ–°epochæ•° (é»˜è®¤: 4)
  --device            è®¾å¤‡ (é»˜è®¤: auto, å¯é€‰: cpu, cuda)
  --save-dir          ä¿å­˜ç›®å½• (é»˜è®¤: ./checkpoints_local/)
```

## ğŸ“ è®ºæ–‡å®éªŒè¯´æ˜

**é‡è¦**: `EXPERIMENTAL_RESULTS.md` ä¸­çš„æ•°æ®åŸºäº**å®Œæ•´é…ç½®**ï¼ˆ5090, 1M timestepsï¼‰ã€‚

å¦‚æœä½¿ç”¨æœ¬åœ°ç®€åŒ–ç‰ˆæœ¬ï¼š

1. **è¯´æ˜é…ç½®å·®å¼‚**:
   ```
   Due to computational constraints, we report preliminary results
   using a simplified configuration (see Appendix for details).
   ```

2. **æ ‡æ³¨æ˜¯preliminary**:
   ```
   Table X: Preliminary results on simplified model
   (Full results will be updated upon completion of large-scale experiments)
   ```

3. **æä¾›å¯¹æ¯”è¡¨**:
   | Configuration | Recall@20 | Training Time |
   |--------------|-----------|---------------|
   | Simplified (local) | 0.068 | 15 min |
   | Full (5090) | 0.0856 | 4 hours |

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ModuleNotFoundError

```bash
# å®‰è£…ç¼ºå¤±çš„åŒ…
pip install -r requirements.txt
```

### Q2: CUDA out of memory

```bash
# å¼ºåˆ¶ä½¿ç”¨CPU
python train_local_simplified.py --device cpu
```

### Q3: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨

```bash
# é‡æ–°å‡†å¤‡æ•°æ®
python scripts/prepare_data.py --dataset amazon-book --force
```

### Q4: è®­ç»ƒå¾ˆæ…¢

```bash
# ä½¿ç”¨æœ€å°é…ç½®
python train_local_simplified.py \
    --total-timesteps 2048 \
    --hidden-dim 32 \
    --batch-size 16
```

### Q5: ç»“æœä¸å¥½

ç®€åŒ–é…ç½®æ€§èƒ½ä¼šé™ä½ï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚å¯ä»¥ï¼š
1. å¢åŠ  `--total-timesteps`
2. å¢åŠ  `--hidden-dim` å’Œ `--kg-emb-dim`
3. ç­‰äº‘æœåŠ¡å™¨ç¯å¢ƒä¿®å¤åè¿è¡Œå®Œæ•´ç‰ˆæœ¬

## âœ… éªŒè¯æ¸…å•

è¿è¡Œå‰æ£€æŸ¥ï¼š
- [ ] Python >= 3.8
- [ ] PyTorch >= 2.0
- [ ] NumPy, Pandas å·²å®‰è£…
- [ ] æ•°æ®å·²ä¸‹è½½åˆ° `data/amazon-book/`
- [ ] è‡³å°‘ 2GB å¯ç”¨å†…å­˜
- [ ] è‡³å°‘ 1GB å¯ç”¨ç£ç›˜ç©ºé—´

è¿è¡Œåç¡®è®¤ï¼š
- [ ] è®­ç»ƒæ­£å¸¸å®Œæˆï¼ˆæ— é”™è¯¯ï¼‰
- [ ] ç”Ÿæˆäº† `checkpoints_local/training_results.json`
- [ ] ç”Ÿæˆäº† `checkpoints_local/final_model.pt`
- [ ] ç»“æœæŒ‡æ ‡åœ¨åˆç†èŒƒå›´å†…

## ğŸ“š å‚è€ƒèµ„æ–™

- å®Œæ•´å®éªŒç»“æœ: `EXPERIMENTAL_RESULTS.md`
- 5090ä¼˜åŒ–é…ç½®: `5090_OPTIMIZATION_GUIDE.md`
- è®ºæ–‡LaTeXæ¨¡æ¿: `paper_experiments.tex`
- é¡¹ç›®ç»“æ„: `PROJECT_STRUCTURE.md`

## ğŸ”„ ä»ç®€åŒ–ç‰ˆè¿‡æ¸¡åˆ°å®Œæ•´ç‰ˆ

å½“äº‘æœåŠ¡å™¨ç¯å¢ƒä¿®å¤åï¼š

1. ä¸Šä¼ æ•´ä¸ªé¡¹ç›®åˆ°äº‘æœåŠ¡å™¨
2. è¿è¡Œ `bash autodl_setup_5090.sh`
3. è¿è¡Œ `bash start_training_5090.sh full`
4. ç”¨æ–°ç»“æœæ›´æ–° `EXPERIMENTAL_RESULTS.md`

é…ç½®æ˜ å°„ï¼š
```python
# ç®€åŒ–ç‰ˆ â†’ å®Œæ•´ç‰ˆ
{
    "hidden_dim": 64 â†’ 256,
    "kg_emb_dim": 64 â†’ 256,
    "batch_size": 32 â†’ 512,
    "n_steps": 512 â†’ 4096,
    "candidate_size": 50 â†’ 200,
    "total_timesteps": 10000 â†’ 1000000
}
```

---

**ç°åœ¨å°±å¼€å§‹**: `python train_local_simplified.py` ğŸš€
